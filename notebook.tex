% Created 2019-05-07 Tue 18:06
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[margin=2cm]{geometry}
\DeclareMathOperator{\sign}{sign}
\setlength{\parindent}{0cm}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\usetikzlibrary{arrows, decorations.markings}
\usetikzlibrary{3d}
\usetikzlibrary{shapes.geometric,decorations.fractals,shadows}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}


\section{Supervised learning}
\label{sec:orgd6433f1}
Method where you train the program by feeding the learning algorithm with a mapping of
inputs to correct outputs.
\subsection{Regression}
\label{sec:org951d529}
Regression is curve fitting: learn a continuous input \(\to\) output mapping from a set of
examples.
\subsection{Classification}
\label{sec:orgb351be3}
Outputs are discrete variables (category labels). Learn a decision boundary that
separates one class from the other. Generally, a confidence is also desired, i.e.,
how sure are we that the input belongs to the chosen category.
\subsection{Training set}
\label{sec:orgfe0d6e5}
The training set is a set of \(m\) \((\vec{X},\, y)\) pairs, where:
\begin{align*}
  \vec{X} \in \mathbb{R}^d & \quad\text{models the input.} \\
  y \in \{0, 1\} & \quad\text{models the output.}
\end{align*}
\subsection{Error function}
\label{sec:orge11bae8}
The loss function for a model \(f: \vec{X} \mapsto y\) parameterized by \(\vec{W}\) applied to a
dataset \(\{ (\vec{X},\, y) \}\) of size \(m\) is:
\[
  L(\vec{W}) = \sum^m_i{ \left(f_{\vec{W}}(\vec{X}_i) - y_i \right)^2 }
\]
\subsection{Perceptron}
\label{sec:orgf70e4d3}
Perceptron is the trivial neural network. The model for a parameter \(\vec{W} = (\text{threshold},\,
   w_1,\, \hdots,\, w_d)\) and inputs of the form \((1,\, x_1,\, \hdots,\, x_d)\) is given by
\[
  f_{\vec{W}}(\vec{X}) = \sign(\vec{W} \vec{X})
\]
Where \(\sign\) is the activation function. \\
If \(x_i\) is evidence for approval, then \(w_i\) should be high. \\
If \(x_i\) is evidence for denial, then \(w_i\) should be low.
\subsubsection{Learning algorithm}
\label{sec:org4145996}
The learning algorithm of the Perceptron is quite simple. The learning rate \(\in (0,\,
    1]\) is used to scale each step. the For a training set \(S = \{ \, (\vec{X}_1,\, y_1),\enspace (\vec{X}_2,\,
    y_2),\enspace \hdots \, \}\)
\begin{itemize}
\item Starting with random weights, then show each sample in sequence repetitively.
\item If the output is correct, do nothing.
\item If the produced output is negative, and the correct output is positive, increase the weights.
\item If the produced output is positive, and the correct output is negative, decrease the weights.
\item The amount to increase/decrease is given by the current sample scaled by the learning rate.
\end{itemize}
\subsection{Error}
\label{sec:orgb93720e}
The error function for a model \(f\) in a \textbf{training} sample is
\[ E_{\text{in}}(f) \]
This function is known and calculable.
\\[10pt]
The error function for a model \(f\) in a \textbf{hypothetical} sample is
\[ E_{\text{out}}(f) \]
This function is \textbf{not} known, and only \textbf{approachable}.
\\[10pt]
A good approximation of \(E_{\text{out}}\)(f) is the error in a \textbf{test} (or \textbf{validation})
sample
\[ E_{\text{val}}(f) \]

Given a model \(f\) in a set of \(M\) models, the bound for the probability of the error
deviation surpassing a given \(\epsilon\) is
\[
  \mathbb{P}\left(\big| E_{\text{in}}(f) - E_{\text{ou}t}(f) \big| > \big\epsilon\right) \leq 2Me^{-2N\big\epsilon^2}
\]
Notably, \(E_{\text{in}}(f)\) and \(E_{\text{out}}(f)\) deviates as \(f\) becomes complex.
\subsubsection{Empirical error minimization}
\label{sec:orgc632312}
During the learning algorithm, always conserve the weights that produce the lower error. \\
This has a disadvantage: It memorizes the training set.
\subsection{Ensemble learning}
\label{sec:orgf1ee182}
Ensemble learning consists in combining several simple models to form a more complex
model.
\begin{description}
\item[{Bagging:}] Each model training with a different dataset
\item[{Boosting:}] Same dataset, but instrumented for each model to mitigate the weakness of
others
\end{description}
\subsection{Learning decision trees}
\label{sec:org855ce07}
Each layer in the tree consists of an attribute that splits the data into subsets that
are ideally disjoint. \\
The entropy of the subsets produced is a measure of how disjoint they are.
\\[5pt]
For a set containing \(p\) positive and \(n\) negatives, the entropy is
\[
  H\left(\frac{p}{p+n}, \frac{n}{p+n} \right) = - \frac{p}{p + n} \log\left( \frac{p}{p + n} \right) 
                                                - \frac{n}{p + n}\log\left( \frac{n}{p + n} \right)
\]
A given attribute \(A\), with \(k\) distinct values, divides the training set \(S\) into
subsets \(S_1, S_2, \hdots, S_k\). \\
The expected entropy remaining after applying \(A\) is
\[
  EH(A) = \sum_{i = 1}^{k} \left[ \frac{p_i + n_i}{p + n} \cdot H\left( \frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i} \right) \right]
\]
The information gain, i.e. the reduction in entropy for \(A\), is
\[
  I(A) = H\left( \frac{p}{p + n}, \frac{n}{p + n} \right) - EH(A)
\]
\subsection{Capacity}
\label{sec:orgca81e73}
The capacity is a measure of when the training error is a good approximation for the
test error.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,
        xlabel near ticks,
        ylabel near ticks,
        xlabel     = {Training dataset size},
        ylabel     = {Error},
        xmin       = 0,
        ymin       = 0,
        ymax       = 15,
        height     = 7cm,
        width      = 10cm,
        xtick      = \empty,
        ytick      = \empty,
        black
      ]
      \addplot [
        samples=200,
        domain=0:8,
        blue
      ] {(ln(200*x + 1)/ln(7)) + 6.3};
      \addplot [
        samples=200,
        domain=0.1:8,
        blue
      ] {1/log2(x + 1) + 10};
      \addplot [
        samples=200,
        domain=0:8,
        red
      ] {log2(x + 1)};
      \addplot [
        samples=200,
        domain=0.1:8,
        red
      ] {1/log10(x/2.5 + 1) + 2};

      \draw [black, dashed] (axis cs: 6.5, 0) |- (axis cs: 6.5, 4);
      \draw [black, dashed] (axis cs: 2.5, 0) |- (axis cs: 2.5, 10.6);
      \node [black] at (7.5, 2.2) {$E_{\text{in}}$};
      \node [black] at (7.5, 4.5) {$E_{\text{out}}$};
      \node [black] at (7.5, 9.3) {$E_{\text{in}}$};
      \node [black] at (7.5, 11.3) {$E_{\text{out}}$};
      \node [blue] at (5.3, 11.3) {simple model};
      \node [red] at (5.3, 5) {complex model};
      \node [black] at (3.3, 8) {capacity};
      \node [black] at (5.7, 0.7) {capacity};

    \end{axis}
  \end{tikzpicture}
\end{figure}
\subsection{Bias and variance}
\label{sec:orga742173}
\textbf{Bias} is the error due to the fact that the set of functions does not contain the
target function.
\\[5pt]
\textbf{Variance} is the error due to the fact that if we had been using another training set
drawn from the same distribution, we would have obtained another function.
\\[5pt]
\textbf{Regularization} is a method for minimizing the training error, as long as it is still a
good approximation for the test error, trading-off accuracy for simplicity.
\subsection{Single layer neural networks}
\label{sec:org36be6db}
Using the sigmoid as the activation function, and the squared-error loss function:
\[
  L(\vec{W}) = \frac{1}{2} \sum_i^m \left( \sigma\left(\vec{W} \vec{X}_i\right) - y_i \right)^2
\]
To find in which direction the weights minimizes \(L\), the gradient is used:
\[
  \nabla L(\vec{W}) = \sum_i^m \Delta \cdot \Psi
\]
Where the delta rule is
\[
  \Delta = \vec{X}_i \cdot \left( \sigma\left(\vec{W}\vec{X}_i\right) - y_i \right)
\]
And the slope of ligistic is
\[
  \Psi = \sigma\left(\vec{W}\vec{X}_i\right) \cdot \left(1 - \sigma\left(\vec{W}\vec{X}_i\right)\right)
\]
\newpage
\subsubsection{Gradient descent algorithm}
\label{sec:orgd214b5b}
The learning rate \(r \in (0,\, 1]\) is used to scale each step.
\begin{enumerate}
\item Starting with random weights.
\item Compute \(\nabla L(\vec{W})\).
\item \(\vec{W} \leftarrow \vec{W} - r \cdot \nabla L(\vec{W}) = \vec{W} - r \cdot \sum\limits_i^m \Delta \Psi\)
\item Repeat steps 2 and 3 until \(\vec{W}\) doesn't change anymore \((10^{-5})\).
\end{enumerate}
After each iteration, \(L(\vec{W})\) should be checked:
\begin{enumerate}
\item If \(L(\vec{W})\) is converging, the learning rate is correct.
\item If \(L(\vec{W})\) is diverging, the learning rate is too large.
\item If \(L(\vec{W})\) is converging slowly, the learning rate too small.
\end{enumerate}
Also, the algorithm needs feature scaling
\[
  x'_i = \frac{x_i - \min(\vec{X})}{\max(\vec{X}) - \min(\vec{X})}
\]
\subsubsection{Stochastic gradient descent}
\label{sec:org2471100}
Instead of inspecting the whole dataset to detect the direction which minimize \(L\), a
single random sample is picked on each step.
\begin{enumerate}
\item Randomly shuffle the training set.
\item Starting with random weights.
\item For each sample \((\vec{X_i}, y_i)\): \(\>\vec{W} \leftarrow \vec{W} - r \cdot \Delta \Psi\)
\item Repeat step 3 until \(\vec{W}\) doesn't change anymore \((10^{-5})\).
\end{enumerate}
Convergence is not so obvious. After each bulk of iterations, e.g. 1000, check \(L(\vec{W})\):
\begin{enumerate}
\item If \(L(\vec{W})\) is converging, the learning rate is correct.
\item If \(L(\vec{W})\) is diverging, the learning rate is too large.
\item If \(L(\vec{W})\) is converging slowly, the learning rate too small.
\end{enumerate}
\subsubsection{Mini batches}
\label{sec:orgf0eb629}
While GD uses all samples in each iteration, SGD uses only one. A possible middle
ground is to use a mini batch of samples in each iteration.
\[
  \vec{W} \leftarrow \vec{W} - r \cdot \frac{1}{b} \sum\limits_i^b \Delta \Psi
\]
Where \(b\) is the batch size, tipically \(10\).
\subsubsection{Regularization}
\label{sec:org34334ca}
To prevent large weights, the norm of the weights is added to the loss function:
\[
  L(\vec{W}) = |\vec{W}| + \frac{1}{2} \sum_i^m \left( \sigma\left(\vec{W} \vec{X}_i\right) - y_i \right)^2
\]
\subsubsection{Early stopping (cross validation)}
\label{sec:org967e4f7}
Other way to improve is to prevent overfitting:
\begin{enumerate}
\item Separate the data into training and validation sets.
\item Minimize \(L(\vec{W})\) on the training set, stopping when \(L(\vec{W})\) on the validation set
stops improving.
\end{enumerate}
\subsection{Multi layered neural networks}
\label{sec:org3cfa806}
This approach introduces one or more hidden layers in the network, each with one or
more neurons. \\
The model for a hidden layer \(h\) is the aggregation of the models of each neuron \(i\) in
the layer.
\[
  y_{h,i} = \sigma \left( \vec{W}_i \, \vec{X}_h \right) \\
\]
The aggregation of the outputs of the layer defines the input for the neurons in the next layer
\[
  X_{h^+} = \left(1,\, y_{h, 1},\, \hdots,\, y_{h, i}\right)
\]
In practice, the layer's weights are aggregated in a matrix, performing the calculation
in a single take. \\
One implication is that the number of neurons in the hidden layers is directly
proportional to the model's complexity.
\subsubsection{Backpropagation}
\label{sec:orgbf66a56}
\begin{enumerate}
\item Starting with random weights.
\item For each sample, calculate the model, and if the result is incorrect:
\begin{enumerate}
\item Calculate \emph{local gradients} for each neuron. \\
For the neuron \(l\) in the last layer \(k\):
\[
  \delta_{k,l} = \sigma'\left( \vec{W}_l \, \vec{X}_k \right) \cdot (y - y_l)
\]
For the hidden neurons, let \(i^+\) be the attached neuron in the next layer:
\[
  \delta_{h,i} = \sigma'\left( \vec{W}_i \, \vec{X}_h \right) \cdot \left( \delta_{h^+,i^+} \,\cdot\, w_{h^+,i^+} \right)
\]
\item Update the weights with the delta rule. \\
Let \(w_{h,i,j}^+\) be the updated weight, \(w_{h,i,j}\) the current weight, and
\(w_{h,i,j}^-\) the previous weight:
\[
  w_{h,i,j}^+ = w_{h,i,j} + \gamma w_{h,i,j}^- + r \cdot \delta \cdot x_{h,i,j}
\]
Where \(\gamma\) is the momentum, a constant defined to prevent local optima.
\end{enumerate}
\end{enumerate}
\subsection{Support Vector Machines}
\label{sec:org83e20c1}
The VC dimension of a model is the higher number of samples for which it can solve \textbf{any}
learning problem. \\
Therefore, the VC dimension is an estimate of the capacity of a model. \\

The VC dimension for a model \(f\) and a training set of size \(n\) is also a bound on the
test error
\[
  L_{\text{test}}(f) \leq L_{\text{train}}(f) + O\left(\sqrt{\frac{\text{VC}(f)}{n}}\right)
\]
To reduce the test error:
\begin{enumerate}
\item Keep the training error low.
\item Minimize \(\text{VC}(f)\).
\end{enumerate}
By limiting the data to a sphere, we can place a bound on the VC dimension. \\
Let \(d\) be the dimensionality of the data, \(D\) the diameter of the sphere, and
\(\rho\) the margin of the model
\[
  \text{VC}(f) \leq \min\left( d, \left\lceil \frac{D^2}{\rho^2} \right\rceil \right)
\]
Therefore, by maximizing \(\rho\), \(\text{VC}(f)\) becomes \textbf{independent of the dimensionality of
the data}.
\subsubsection{Kernels}
\label{sec:org3401991}
A kernel allows one to map the entries to a higher dimensional feature space, possibly
allowing simpler ways to delimit such entries. \\
One example is the polynomial kernel:
\[
  \left(\vec{x} \cdot \vec{y}\right)^n
\]
\subsection{Neural networks versus SVMs}
\label{sec:orge10f646}
\begin{enumerate}
\item Linear SVMs are similar to a Perceptron, but with an optimal cost function.
\item If a Kernel is used, then SVMs are comparable to 2-layer neural networks.
\item A 3-layer neural network might correspond to an ensemble of multiple Kernel SVMs.
\end{enumerate}
\subsection{Naive Bayes}
\label{sec:orga4df332}
Assuming conditional independence between the input dimensions, the probability of the
target can be approximated using the Bayes theorem:
\[
  P\big(y \>|\> x_1, \hdots, x_d \big) \approx P(y) \cdot \prod_{i}^{d} P\big( x_i \>|\> y \big)
\]
\newpage
\subsection{Boosting}
\label{sec:org68dcccf}
Boosting is the technique of combining simple models to create a complex model. \\

One simple method of boosting is the \textbf{additive boosting}: \\
Considering binary classifiers
\begin{align*}
  & h: \vec{X} \mapsto y \\
  & y \in \{ -1, 1 \}
\end{align*}
The model is defined as
\[
h(\vec{X}) = \sign\big(h_1(\vec{X}) + \hdots + h_n(\vec{X})\big)
\]
\subsubsection{Adaboost}
\label{sec:orge3831d6}
The adaptive boosting algorithm is an additive algorithm, with associated importances:
\[
  h(X) = \sign\big(\alpha_1 \cdot h_1(X) + \hdots + \alpha_n \cdot h_n(X)\big)
\]
The adaboost algorithm is \textbf{always based on very simple models}, usually decision
stumps. \\
As a consequence, it \textbf{does not overfit}.
\section{Reinforcement learning}
\label{sec:org7e01a9e}
Method where you train the program by rewarding the learning algorithm positively or
negatively according to the produced results. This method is similar to how we teach
animals.
\section{Unsupervised learning}
\label{sec:org53f621e}
Given only inputs as training, find a pattern: discover clusters, manifolds, embedding.
\end{document}
