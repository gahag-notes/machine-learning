# -*- after-save-hook: org-latex-export-to-pdf; -*-
#+latex_header: \usepackage[margin=2cm]{geometry}
#+latex_header: \DeclareMathOperator{\sign}{sign}
#+latex_header: \setlength{\parindent}{0cm}
#+latex_header: \usepackage{pgfplots}
#+latex_header: \pgfplotsset{compat=1.11}
#+latex_header: \usetikzlibrary{arrows, decorations.markings}
#+latex_header: \usetikzlibrary{3d}
#+latex_header: \usetikzlibrary{shapes.geometric,decorations.fractals,shadows}

* Supervised learning
  Method where you train the program by feeding the learning algorithm with a mapping of
  inputs to correct outputs.
** Regression
   Regression is curve fitting: learn a continuous input $\to$ output mapping from a set of
   examples.
** Classification
   Outputs are discrete variables (category labels). Learn a decision boundary that
   separates one class from the other. Generally, a confidence is also desired, i.e.,
   how sure are we that the input belongs to the chosen category.
** Training set
   The training set is a set of $m$ $(\vec{X},\, y)$ pairs, where:
   #+begin_export latex
   \begin{align*}
     \vec{X} \in \mathbb{R}^d & \quad\text{models the input.} \\
     y \in \{0, 1\} & \quad\text{models the output.}
   \end{align*}
   #+end_export
** Error function
   The error function for a model $f: \vec{X} \mapsto y$ parameterized by $\vec{W}$ applied to a
   dataset $\{ (\vec{X},\, y) \}$ of size $m$ is:
   #+begin_export latex
   \[
     \min_{\vec{W}} \left[ L(\vec{W}) = \sum^m_i{ \big(f_{\vec{W}}(\vec{X}_i) - y_i \big)^2 } \right]
   \]
   #+end_export
** Perceptron
   Perceptron is the trivial neural network. The model for a parameter $\vec{W} = (\text{threshold},\,
   w_1,\, \hdots,\, w_d)$ and inputs of the form $(1,\, x_1,\, \hdots,\, x_d)$ is given by
   #+begin_export latex
   \[
     f_{\vec{W}}(\vec{X}) = \sign(\vec{W} \vec{X})
   \]
   #+end_export
   Where $\sign$ is the activation function. \\
   If $x_i$ is evidence for approval, then $w_i$ should be high. \\
   If $x_i$ is evidence for denial, then $w_i$ should be low.
*** Learning algorithm
    The learning algorithm of the Perceptron is quite simple. The learning rate $\in (0,\,
    1]$ is used to scale each step. the For a training set $S = \{ \, (\vec{X}_1,\, y_1),\enspace (\vec{X}_2,\,
    y_2),\enspace \hdots \, \}$
    - Starting with random weights, then show each sample in sequence repetitively.
    - If the output is correct, do nothing.
    - If the produced output is negative, and the correct output is positive, increase the weights.
    - If the produced output is positive, and the correct output is negative, decrease the weights.
    - The amount to increase/decrease is given by the current sample scaled by the learning rate.
** Error
   The error function for a model $f$ in a *training* sample is
   #+begin_export latex
   \[ E_{\text{in}}(f) \]
   #+end_export
   This function is known and calculable.
   @@latex:\\[10pt]@@
   The error function for a model $f$ in a *test* sample is
   #+begin_export latex
   \[ E_{\text{ou}t}(f) \]
   #+end_export
   This function is *not* known, and only *approachable*.
   @@latex:\\[10pt]@@
   Given a model $f$ in a set of $M$ models, the bound for the probability of the error
   deviation surpassing a given $\epsilon$ is
   #+begin_export latex
   \[
     \mathbb{P}\left(\big| E_{\text{in}}(f) - E_{\text{ou}t}(f) \big| > \big\epsilon\right) \leq 2Me^{-2N\big\epsilon^2}
   \]
   #+end_export
   Notably, $E_{\text{in}}(f)$ and $E_{\text{out}}(f)$ deviates as $f$ becomes complex.
*** Empirical error minimization
    During the learning algorithm, always conserve the weights that produce the lower error. \\
    This has a disadvantage: It memorizes the training set.
** Ensemble learning
   Ensemble learning consists in combining several simple models to form a more complex
   model.
   - Bagging: :: Each model training with a different dataset
   - Boosting: :: Same dataset, but instrumented for each model to mitigate the weakness of
                 others
** Learning decision trees
   Each layer in the tree consists of an attribute that splits the data into subsets that
   are ideally disjoint. \\
   The entropy of the subsets produced is a measure of how disjoint they are.
   @@latex:\\[5pt]@@
   For a set containing $p$ positive and $n$ negatives, the entropy is
   #+begin_export latex
   \[
     H\left(\frac{p}{p+n}, \frac{n}{p+n} \right) = - \frac{p}{p + n} \log\left( \frac{p}{p + n} \right) 
                                                   - \frac{n}{p + n}\log\left( \frac{n}{p + n} \right)
   \]
   #+end_export
   A given attribute $A$, with $k$ distinct values, divides the training set $S$ into
   subsets $S_1, S_2, \hdots, S_k$. \\
   The expected entropy remaining after applying $A$ is
   #+begin_export latex
   \[
     EH(A) = \sum_{i = 1}^{k} \left[ \frac{p_i + n_i}{p + n} \cdot H\left( \frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i} \right) \right]
   \]
   #+end_export
   The information gain, i.e. the reduction in entropy for $A$, is
   #+begin_export latex
   \[
     I(A) = H\left( \frac{p}{p + n}, \frac{n}{p + n} \right) - EH(A)
   \]
   #+end_export
** Capacity
   The capacity is a measure of when the training error is a good approximation for the
   test error.
   #+begin_export latex
   \begin{figure}[H]
     \centering
     \begin{tikzpicture}
       \begin{axis}[
           axis lines = middle,
           xlabel near ticks,
           ylabel near ticks,
           xlabel     = {Training dataset size},
           ylabel     = {Error},
           xmin       = 0,
           ymin       = 0,
           ymax       = 15,
           height     = 7cm,
           width      = 10cm,
           xtick      = \empty,
           ytick      = \empty,
           black
         ]
         \addplot [
           samples=200,
           domain=0:8,
           blue
         ] {(ln(200*x + 1)/ln(7)) + 6.3};
         \addplot [
           samples=200,
           domain=0.1:8,
           blue
         ] {1/log2(x + 1) + 10};
         \addplot [
           samples=200,
           domain=0:8,
           red
         ] {log2(x + 1)};
         \addplot [
           samples=200,
           domain=0.1:8,
           red
         ] {1/log10(x/2.5 + 1) + 2};

         \draw [black, dashed] (axis cs: 6.5, 0) |- (axis cs: 6.5, 4);
         \draw [black, dashed] (axis cs: 2.5, 0) |- (axis cs: 2.5, 10.6);
         \node [black] at (7.5, 2.2) {$E_{\text{in}}$};
         \node [black] at (7.5, 4.5) {$E_{\text{out}}$};
         \node [black] at (7.5, 9.3) {$E_{\text{in}}$};
         \node [black] at (7.5, 11.3) {$E_{\text{out}}$};
         \node [blue] at (5.3, 11.3) {simple model};
         \node [red] at (5.3, 5) {complex model};
         \node [black] at (3.3, 8) {capacity};
         \node [black] at (5.7, 0.7) {capacity};

       \end{axis}
     \end{tikzpicture}
   \end{figure}
   #+end_export
** Bias and variance
   *Bias* is the error due to the fact that the set of functions does not contain the
   target function.
   @@latex:\\[5pt]@@
   *Variance* is the error due to the fact that if we had been using another training set
   drawn from the same distribution, we would have obtained another function.
   @@latex:\\[5pt]@@
   *Regularization* is a method for minimizing the training error, as long as it is still a
   good approximation for the test error, trading-off accuracy for simplicity.
** Single layer neural networks
   Using the sigmoid as the activation function, and the squared-error loss function:
   #+begin_export latex
   \[
     L(\vec{W}) = \frac{1}{2} \sum_i^m \left( \sigma\left(\vec{W} \vec{X}_i\right) - y_i \right)^2
   \]
   #+end_export
   To find in which direction the weights minimizes $L$, the gradient is used:
   #+begin_export latex
   \[
     \nabla L(\vec{W}) = \sum_i^m \Delta \cdot \Psi
   \]
   #+end_export
   Where the delta rule is
   #+begin_export latex
   \[
     \Delta = \vec{X}_i \cdot \left( \sigma\left(\vec{W}\vec{X}_i\right) - y_i \right)
   \]
   #+end_export
   And the slope of ligistic is
   #+begin_export latex
   \[
     \Psi = \sigma\left(\vec{W}\vec{X}_i\right) \cdot \left(1 - \sigma\left(\vec{W}\vec{X}_i\right)\right)
   \]
   #+end_export
   @@latex:\newpage@@
*** Gradient descent algorithm
    The learning rate $r \in (0,\, 1]$ is used to scale each step.
    1. Starting with random weights.
    2. Compute $\nabla L(\vec{W})$.
    3. $\vec{W} \leftarrow \vec{W} - r \cdot \nabla L(\vec{W}) = \vec{W} - r \cdot \sum\limits_i^m \Delta \Psi$
    4. Repeat steps 2 and 3 until $\vec{W}$ doesn't change anymore $(10^{-5})$.
    After each iteration, $L(\vec{W})$ should be checked:
    1. If $L(\vec{W})$ is converging, the learning rate is correct.
    2. If $L(\vec{W})$ is diverging, the learning rate is too large.
    3. If $L(\vec{W})$ is converging slowly, the learning rate too small.
    Also, the algorithm needs feature scaling
    #+begin_export latex
    \[
      x'_i = \frac{x_i - \min(\vec{X})}{\max(\vec{X}) - \min(\vec{X})}
    \]
    #+end_export
*** Stochastic gradient descent
    Instead of inspecting the whole dataset to detect the direction which minimize $L$, a
    single random sample is picked on each step.
    1. Randomly shuffle the training set.
    2. Starting with random weights.
    3. For each sample $(\vec{X_i}, y_i)$: $\>\vec{W} \leftarrow \vec{W} - r \cdot \Delta \Psi$
    4. Repeat step 3 until $\vec{W}$ doesn't change anymore $(10^{-5})$.
    Convergence is not so obvious. After each bulk of iterations, e.g. 1000, check $L(\vec{W})$:
    1. If $L(\vec{W})$ is converging, the learning rate is correct.
    2. If $L(\vec{W})$ is diverging, the learning rate is too large.
    3. If $L(\vec{W})$ is converging slowly, the learning rate too small.
*** Mini batches
    While GD uses all samples in each iteration, SGD uses only one. A possible middle
    ground is to use a mini batch of samples in each iteration.
    #+begin_export latex
    \[
      \vec{W} \leftarrow \vec{W} - r \cdot \frac{1}{b} \sum\limits_i^b \Delta \Psi
    \]
    #+end_export
    Where $b$ is the batch size, tipically $10$.
*** Regularization
    To prevent large weights, the norm of the weights is added to the loss function:
    #+begin_export latex
    \[
      L(\vec{W}) = |\vec{W}| + \frac{1}{2} \sum_i^m \left( \sigma\left(\vec{W} \vec{X}_i\right) - y_i \right)^2
    \]
    #+end_export
*** Early stopping (cross validation)
    Other way to improve is to prevent overfitting:
    1. Separate the data into training and validation sets.
    2. Minimize $L(\vec{W})$ on the training set, stopping when $L(\vec{W})$ on the validation set
       stops improving.
* Reinforcement learning
  Method where you train the program by rewarding the learning algorithm positively or
  negatively according to the produced results. This method is similar to how we teach
  animals.
* Unsupervised learning
  Given only inputs as training, find a pattern: discover clusters, manifolds, embedding.
