# -*- after-save-hook: org-latex-export-to-pdf; -*-
#+latex_header: \usepackage[margin=2cm]{geometry}
#+latex_header: \usepackage{enumitem}
#+latex_header: \DeclareMathOperator{\sign}{sign}
#+latex_header: \setlength{\parindent}{0cm}
#+latex_header: \usepackage{pgfplots}
#+latex_header: \pgfplotsset{compat=1.11}
#+latex_header: \usetikzlibrary{arrows, decorations.markings}
#+latex_header: \usetikzlibrary{3d}
#+latex_header: \usetikzlibrary{shapes.geometric,decorations.fractals,shadows}

* Supervised learning
  Method where you train the program by feeding the learning algorithm with a mapping of
  inputs to correct outputs.
** Regression
   Regression is curve fitting: learn a continuous input $\to$ output mapping from a set of
   examples.
** Classification
   Outputs are discrete variables (category labels). Learn a decision boundary that
   separates one class from the other. Generally, a confidence is also desired, i.e.,
   how sure are we that the input belongs to the chosen category.
** Training set
   The training set is a set of $m$ $(\vec{X},\, y)$ pairs, where:
   #+begin_export latex
   \begin{align*}
     \vec{X} \in \mathbb{R}^d & \quad\text{models the input.} \\
     y \in \{0, 1\} & \quad\text{models the output.}
   \end{align*}
   #+end_export
** Error function
   The loss function for a model $f: \vec{X} \mapsto y$ parameterized by $\vec{W}$ applied to a
   dataset $\{ (\vec{X},\, y) \}$ of size $m$ is:
   #+begin_export latex
   \[
     L(\vec{W}) = \sum^m_i{ \left(f_{\vec{W}}(\vec{X}_i) - y_i \right)^2 }
   \]
   #+end_export
** Perceptron
   Perceptron is the trivial neural network. The model for a parameter $\vec{W} = (\text{threshold},\,
   w_1,\, \hdots,\, w_d)$ and inputs of the form $(1,\, x_1,\, \hdots,\, x_d)$ is given by
   #+begin_export latex
   \[
     f_{\vec{W}}(\vec{X}) = \sign(\vec{W} \vec{X})
   \]
   #+end_export
   Where $\sign$ is the activation function. \\
   If $x_i$ is evidence for approval, then $w_i$ should be high. \\
   If $x_i$ is evidence for denial, then $w_i$ should be low.
*** Learning algorithm
    The learning algorithm of the Perceptron is quite simple. The learning rate $\in (0,\,
    1]$ is used to scale each step. the For a training set $S = \{ \, (\vec{X}_1,\, y_1),\enspace (\vec{X}_2,\,
    y_2),\enspace \hdots \, \}$
    #+attr_latex: :options [itemsep=0pt]
    - Starting with random weights, then show each sample in sequence repetitively.
    - If the output is correct, do nothing.
    - If the produced output is negative, and the correct output is positive, increase the weights.
    - If the produced output is positive, and the correct output is negative, decrease the weights.
    - The amount to increase/decrease is given by the current sample scaled by the learning rate.
** Error
   The error function for a model $f$ in a *training* sample is
   #+begin_export latex
   \[ E_{\text{in}}(f) \]
   #+end_export
   This function is known and calculable.
   @@latex:\\[10pt]@@
   The error function for a model $f$ in a *hypothetical* sample is
   #+begin_export latex
   \[ E_{\text{out}}(f) \]
   #+end_export
   This function is *not* known, and only *approachable*.
   @@latex:\\[10pt]@@
   A good approximation of $E_{\text{out}}$(f) is the error in a *test* (or *validation*)
   sample
   #+begin_export latex
   \[ E_{\text{val}}(f) \]
   #+end_export
   @@latex:\\@@
   Given a model $f$ in a set of $M$ models, the bound for the probability of the error
   deviation surpassing a given $\epsilon$ is
   #+begin_export latex
   \[
     \mathbb{P}\left(\big| E_{\text{in}}(f) - E_{\text{ou}t}(f) \big| > \big\epsilon\right) \leq 2Me^{-2N\big\epsilon^2}
   \]
   #+end_export
   Notably, $E_{\text{in}}(f)$ and $E_{\text{out}}(f)$ deviates as $f$ becomes complex.
*** Empirical error minimization
    During the learning algorithm, always conserve the weights that produce the lower error. \\
    This has a disadvantage: It memorizes the training set.
** Learning decision trees
   Each layer in the tree consists of an attribute that splits the data into subsets that
   are ideally disjoint. \\
   The entropy of the subsets produced is a measure of how disjoint they are.
   @@latex:\\[5pt]@@
   For a set containing $p$ positive and $n$ negatives, the entropy is
   #+begin_export latex
   \[
     H\left(\frac{p}{p+n}, \frac{n}{p+n} \right) = - \frac{p}{p + n} \log\left( \frac{p}{p + n} \right) 
                                                   - \frac{n}{p + n}\log\left( \frac{n}{p + n} \right)
   \]
   #+end_export
   A given attribute $A$, with $k$ distinct values, divides the training set $S$ into
   subsets $S_1, S_2, \hdots, S_k$. \\
   The expected entropy remaining after applying $A$ is
   #+begin_export latex
   \[
     EH(A) = \sum_{i = 1}^{k} \left[ \frac{p_i + n_i}{p + n} \cdot H\left( \frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i} \right) \right]
   \]
   #+end_export
   The information gain, i.e. the reduction in entropy for $A$, is
   #+begin_export latex
   \[
     I(A) = H\left( \frac{p}{p + n}, \frac{n}{p + n} \right) - EH(A)
   \]
   #+end_export
** Capacity
   The capacity is a measure of when the training error is a good approximation for the
   test error.
   #+begin_export latex
   \begin{figure}[H]
     \centering
     \begin{tikzpicture}
       \begin{axis}[
           axis lines = middle,
           xlabel near ticks,
           ylabel near ticks,
           xlabel     = {Training dataset size},
           ylabel     = {Error},
           xmin       = 0,
           ymin       = 0,
           ymax       = 15,
           height     = 7cm,
           width      = 10cm,
           xtick      = \empty,
           ytick      = \empty,
           black
         ]
         \addplot [
           samples=200,
           domain=0:8,
           blue
         ] {(ln(200*x + 1)/ln(7)) + 6.3};
         \addplot [
           samples=200,
           domain=0.1:8,
           blue
         ] {1/log2(x + 1) + 10};
         \addplot [
           samples=200,
           domain=0:8,
           red
         ] {log2(x + 1)};
         \addplot [
           samples=200,
           domain=0.1:8,
           red
         ] {1/log10(x/2.5 + 1) + 2};

         \draw [black, dashed] (axis cs: 6.5, 0) |- (axis cs: 6.5, 4);
         \draw [black, dashed] (axis cs: 2.5, 0) |- (axis cs: 2.5, 10.6);
         \node [black] at (7.5, 2.2) {$E_{\text{in}}$};
         \node [black] at (7.5, 4.5) {$E_{\text{out}}$};
         \node [black] at (7.5, 9.3) {$E_{\text{in}}$};
         \node [black] at (7.5, 11.3) {$E_{\text{out}}$};
         \node [blue] at (5.3, 11.3) {simple model};
         \node [red] at (5.3, 5) {complex model};
         \node [black] at (3.3, 8) {capacity};
         \node [black] at (5.7, 0.7) {capacity};

       \end{axis}
     \end{tikzpicture}
   \end{figure}
   #+end_export
** Bias and variance
   *Bias* is the error due to the fact that the set of functions does not contain the
   target function.
   @@latex:\\[5pt]@@
   *Variance* is the error due to the fact that if we had been using another training set
   drawn from the same distribution, we would have obtained another function.
   @@latex:\\[5pt]@@
   *Regularization* is a method for minimizing the training error, as long as it is still a
   good approximation for the test error, trading-off accuracy for simplicity.
** Single layer neural networks
   Using the sigmoid as the activation function, and the squared-error loss function:
   #+begin_export latex
   \[
     L(\vec{W}) = \frac{1}{2} \sum_i^m \left( \sigma\left(\vec{W} \vec{X}_i\right) - y_i \right)^2
   \]
   #+end_export
   To find in which direction the weights minimizes $L$, the gradient is used:
   #+begin_export latex
   \[
     \nabla L(\vec{W}) = \sum_i^m \Delta \cdot \Psi
   \]
   #+end_export
   Where the delta rule is
   #+begin_export latex
   \[
     \Delta = \vec{X}_i \cdot \left( \sigma\left(\vec{W}\vec{X}_i\right) - y_i \right)
   \]
   #+end_export
   And the slope of ligistic is
   #+begin_export latex
   \[
     \Psi = \sigma\left(\vec{W}\vec{X}_i\right) \cdot \left(1 - \sigma\left(\vec{W}\vec{X}_i\right)\right)
   \]
   #+end_export
   @@latex:\newpage@@
*** Gradient descent algorithm
    The learning rate $r \in (0,\, 1]$ is used to scale each step.
    1. Starting with random weights.
    2. Compute $\nabla L(\vec{W})$.
    3. $\vec{W} \leftarrow \vec{W} - r \cdot \nabla L(\vec{W}) = \vec{W} - r \cdot \sum\limits_i^m \Delta \Psi$
    4. Repeat steps 2 and 3 until $\vec{W}$ doesn't change anymore $(10^{-5})$.
    After each iteration, $L(\vec{W})$ should be checked:
    1. If $L(\vec{W})$ is converging, the learning rate is correct.
    2. If $L(\vec{W})$ is diverging, the learning rate is too large.
    3. If $L(\vec{W})$ is converging slowly, the learning rate too small.
    Also, the algorithm needs feature scaling
    #+begin_export latex
    \[
      x'_i = \frac{x_i - \min(\vec{X})}{\max(\vec{X}) - \min(\vec{X})}
    \]
    #+end_export
*** Stochastic gradient descent
    Instead of inspecting the whole dataset to detect the direction which minimize $L$, a
    single random sample is picked on each step.
    1. Randomly shuffle the training set.
    2. Starting with random weights.
    3. For each sample $(\vec{X_i}, y_i)$: $\>\vec{W} \leftarrow \vec{W} - r \cdot \Delta \Psi$
    4. Repeat step 3 until $\vec{W}$ doesn't change anymore $(10^{-5})$.
    Convergence is not so obvious. After each bulk of iterations, e.g. 1000, check $L(\vec{W})$:
    1. If $L(\vec{W})$ is converging, the learning rate is correct.
    2. If $L(\vec{W})$ is diverging, the learning rate is too large.
    3. If $L(\vec{W})$ is converging slowly, the learning rate too small.
*** Mini batches
    While GD uses all samples in each iteration, SGD uses only one. A possible middle
    ground is to use a mini batch of samples in each iteration.
    #+begin_export latex
    \[
      \vec{W} \leftarrow \vec{W} - r \cdot \frac{1}{b} \sum\limits_i^b \Delta \Psi
    \]
    #+end_export
    Where $b$ is the batch size, tipically $10$.
*** Regularization
    To prevent large weights, the norm of the weights is added to the loss function:
    #+begin_export latex
    \[
      L(\vec{W}) = |\vec{W}| + \frac{1}{2} \sum_i^m \left( \sigma\left(\vec{W} \vec{X}_i\right) - y_i \right)^2
    \]
    #+end_export
*** Early stopping (cross validation)
    Other way to improve is to prevent overfitting:
    #+attr_latex: :options [itemsep=0pt]
    1. Separate the data into training and validation sets.
    2. Minimize $L(\vec{W})$ on the training set, stopping when $L(\vec{W})$ on the validation set
       stops improving.
** Multi layered neural networks
   This approach introduces one or more hidden layers in the network, each with one or
   more neurons. \\
   The model for a hidden layer $h$ is the aggregation of the models of each neuron $i$ in
   the layer.
   #+begin_export latex
   \[
     y_{h,i} = \sigma \left( \vec{W}_i \, \vec{X}_h \right) \\
   \]
   #+end_export
   The aggregation of the outputs of the layer defines the input for the neurons in the next layer
   #+begin_export latex
   \[
     X_{h^+} = \left(1,\, y_{h, 1},\, \hdots,\, y_{h, i}\right)
   \]
   #+end_export
   In practice, the layer's weights are aggregated in a matrix, performing the calculation
   in a single take. \\
   One implication is that the number of neurons in the hidden layers is directly
   proportional to the model's complexity.
*** Backpropagation
    1. Starting with random weights.
    2. For each sample, calculate the model, and if the result is incorrect:
       a. Calculate /local gradients/ for each neuron. \\
          For the neuron $l$ in the last layer $k$:
          #+begin_export latex
          \[
            \delta_{k,l} = \sigma'\left( \vec{W}_l \, \vec{X}_k \right) \cdot (y - y_l)
          \]
          #+end_export
          For the hidden neurons, let $i^+$ be the attached neuron in the next layer:
          #+begin_export latex
          \[
            \delta_{h,i} = \sigma'\left( \vec{W}_i \, \vec{X}_h \right) \cdot \left( \delta_{h^+,i^+} \,\cdot\, w_{h^+,i^+} \right)
          \]
          #+end_export
       b. Update the weights with the delta rule. \\
          Let $w_{h,i,j}^+$ be the updated weight, $w_{h,i,j}$ the current weight, and
          $w_{h,i,j}^-$ the previous weight:
          #+begin_export latex
          \[
            w_{h,i,j}^+ = w_{h,i,j} + \gamma w_{h,i,j}^- + r \cdot \delta \cdot x_{h,i,j}
          \]
          #+end_export
          Where $\gamma$ is the momentum, a constant defined to prevent local optima.
** Support Vector Machines
   The VC dimension of a model is the higher number of samples for which it can solve *any*
   learning problem. \\
   Therefore, the VC dimension is an estimate of the capacity of a model. \\

   The VC dimension for a model $f$ and a training set of size $n$ is also a bound on the
   test error
   #+begin_export latex
   \[
     L_{\text{test}}(f) \leq L_{\text{train}}(f) + O\left(\sqrt{\frac{\text{VC}(f)}{n}}\right)
   \]
   #+end_export
   To reduce the test error:
   #+attr_latex: :options [itemsep=0pt]
   1. Keep the training error low.
   2. Minimize $\text{VC}(f)$.
   By limiting the data to a sphere, we can place a bound on the VC dimension. \\
   Let $d$ be the dimensionality of the data, $D$ the diameter of the sphere, and
   $\rho$ the margin of the model
   #+begin_export latex
   \[
     \text{VC}(f) \leq \min\left( d, \left\lceil \frac{D^2}{\rho^2} \right\rceil \right)
   \]
   #+end_export
   Therefore, by maximizing $\rho$, $\text{VC}(f)$ becomes *independent of the dimensionality of
   the data*.
*** Kernels
    A kernel allows one to map the entries to a higher dimensional feature space, possibly
    allowing simpler ways to delimit such entries. \\
    One example is the polynomial kernel:
    #+begin_export latex
    \[
      \left(\vec{x} \cdot \vec{y}\right)^n
    \]
    #+end_export
** Neural networks versus SVMs
   #+attr_latex: :options [itemsep=0pt]
   1. Linear SVMs are similar to a Perceptron, but with an optimal cost function.
   2. If a Kernel is used, then SVMs are comparable to 2-layer neural networks.
   3. A 3-layer neural network might correspond to an ensemble of multiple Kernel SVMs.
** Naive Bayes
   Assuming conditional independence between the input dimensions, the probability of the
   target can be approximated using the Bayes theorem:
   #+begin_export latex
   \[
     P\big(y \>|\> x_1, \hdots, x_d \big) \approx P(y) \cdot \prod_{i}^{d} P\big( x_i \>|\> y \big)
   \]
   #+end_export
   @@latex:\newpage@@
** Ensemble learning
   Ensemble learning consists in combining several simple models to form a more complex
   model.
   - Bagging: :: Each model training with a different dataset
   - Boosting: :: Same dataset, but instrumented for each model to mitigate the weakness of
                 others
** Boosting
   Boosting is the technique of combining simple models iteratively to create a complex
   model. \\
   Each model is intentionally *biased* to avoid the errors of the previous model.
   @@latex:\\@@ \\
   One simple method of boosting is the *additive boosting*: \\
   Considering binary classifiers
   #+begin_export latex
   \begin{align*}
     & h: \vec{X} \mapsto y \\
     & y \in \{ -1, 1 \}
   \end{align*}
   #+end_export
   The model is defined as
   #+begin_export latex
   \[
   h(\vec{X}) = \sign\big(h_1(\vec{X}) + \hdots + h_n(\vec{X})\big)
   \]
   #+end_export
*** Adaboost
    The adaptive boosting algorithm is an additive algorithm, with associated importances:
    #+begin_export latex
    \[
      h(X) = \sign\big(\alpha_1 \cdot h_1(X) + \hdots + \alpha_n \cdot h_n(X)\big)
    \]
    #+end_export
    The adaboost algorithm is *always based on very simple models*, usually decision
    stumps. \\
    As a consequence, it *does not overfit*.
** Bagging
   Boostrap aggregation is the technique of combining models trained in subsets of the
   training dataset. \\
   The subsets are constructed by uniformly sampling the dataset, and may contain
   intersections. \\
   *Small* subsets *prevent* the base models from *overfitting*, and therefore bagging
   circumvents *variance* in the data. @@latex:\\@@ \\
   The models may be combined using many techniques:
   #+attr_latex: :options [itemsep=0pt]
   - Majority voting.
   - Averaging probabilites.
   - Averaging estimates.
   - Etc.
   In practice, the base models are usually decision trees.
*** Random forests
    Random forests exploits randomness in instances and features. \\
    Each decision tree is trained with a random subset of *features* and instances. @@latex:\\@@
    As a consequence, random forests circumvent overfitting in decision trees.
** Semi-supervised learning
   Semi-supervised learning is the method of combining supervised and unsupervised
   learning, usually when there are small ammounts of labeled data, and large amounts of
   unlabeled data.
*** Active learning
    Active learning is a technique to create optimal training sets, by filtering samples
    with redundant information. @@latex:\\@@ \\
    The technique is commonly used in two situations:
    #+attr_latex: :options [itemsep=0pt]
    - Semi-supervised: :: use active learning to obtain a small optimal subset of samples
         to label manually.
    - Supervised: :: use active learning to balance classes, e.g. for classifying
                     anomalies, where there are few positive samples.
    @@latex:\vspace{5px}@@  There are many methods for selecting samples:
    - Uncertainty sampling: :: Select the samples for which the model is least certain
         about the correct output.
    - Committee: :: Train a comittee of models on the labeled data. Select the unlabeled
                    samples for which the committee disagrees most.
    - Expected model change: :: Select the samples with greater impact on the model.
    - Expected error reduction: :: Select the samples with greater impact on the model's
         generalization error.
* Unsupervised learning
  Unsupervised learning consists to, given only inputs as training, find a pattern:
  #+attr_latex: :options [itemsep=0pt]
  - Clusters
  - Manifolds
  - Embeddings
  - Etc.
** Distance function
   Some common distance functions are:
   - Nearest neighbor: :: $\min({|x - y|}^2)$
   - Furthest neighbor: :: $\max({|x - y|}^2)$
   - Centroid: :: ${|\mu_i - \mu_j|}^2$
** Hierarchical agglomerative clustering
   The hierarchical agglomerative clustering technique constructs a dendogram based on a
   distance function. \\
   Starting with individual clusters, it iteratively merges the closest ones until the
   dendogram is complete. Finally, a cut across the dendogram corresponds to a similarity
   threshold.
** K-Means
   The K-Means algorithm constructs clusters by placing centroids and agglomerating by the
   closest centroid. \\
   Considering $k$ clusters, place $k$ centroids in the space. Update the centroids
   iteratively using an expectation-maximization algorithm:
   #+attr_latex: :options [itemsep=0pt]
   - Each cluster is defined by the points that are closest to the correspondent centroid.
   - The centroids are updated with the mean of the points in it's cluster.
   This technique can be interpreted as optimizing a loss function
   #+begin_export latex
   \[
     L = \sum_i \left| x_i - \mu_j \right|^2
   \]
   #+end_export
   Different initial centroids may lead to different final losses, motivating different
   initialization methods:
   #+attr_latex: :options [itemsep=0pt]
   - Random: :: May choose nearby points
   - Distance based: :: Limits the search space, not enough randomness.
   - Random and distance based: :: Choose far points randomly.
   The choice of $k$ also impacts the disposition of results:
   #+attr_latex: :options [itemsep=0pt]
   - Small $k$: :: loose clustering.
   - Large $k$: :: any point is a group itself.
   To prevent an overly large $k$, we must penalize complexity (regularization):
   #+begin_export latex
   \[
     L = \log \left( \frac{1}{n \cdot d} \cdot \sum_i \left| x_i - \mu_j \right|^2 \right) + k \cdot \frac{\log{n}}{n}
   \]
   #+end_export
   Where $d$ is the dimensionality of the data, which also plays a fundamental role. With
   more dimensions, points tend to get sparse. Therefore, the more dimensions there are,
   the more samples one will need.
** Mixture model
   The mixture model constructs clusters by assigning probabily distributions and
   agglomerating by the most probable distribution. Therefore, it differs from K-Means by
   allowing overlaping clusters. \\
   As in the K-Means algorithm, the distributions are randomly initialized, and updated
   using an expectation-maximization algorithm.
* Reinforcement learning
  Method where you train the program by rewarding the learning algorithm positively or
  negatively according to the produced results. This method is similar to how we teach
  animals.
